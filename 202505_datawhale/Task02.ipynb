{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6304638f-9194-43fc-8ccc-61bc855be389",
   "metadata": {},
   "source": [
    "# Task02. æ­å»ºå¼€å‘ç¯å¢ƒå¹¶è¿è¡Œã€ç†è§£æ—¶åºæ’è¡¥å·¥ä½œæµ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b1d8f-2dc4-41a3-a799-5d0e4278d686",
   "metadata": {},
   "source": [
    "## 1. å¼€å‘ç¯å¢ƒé…ç½®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b89c6d-be99-48d8-93ae-670be29802a3",
   "metadata": {},
   "source": [
    "### PyPOTSå¼€å‘ç¯å¢ƒæ”¯æŒå¤šç§å®‰è£…æ–¹å¼, ä½ å¯ä»¥è‡ªç”±é€‰æ‹©ä»æºç å®‰è£…, é€šè¿‡pipå®‰è£…PyPIä¸Šçš„å‘å¸ƒç‰ˆæœ¬æˆ–è€…ä½¿ç”¨condaä»conda-forgeçš„å‘è¡Œç‰ˆè¿›è¡Œç¯å¢ƒé…ç½®, å¦‚æœä½ ç†Ÿæ‚‰dockerçš„ä½¿ç”¨æ–¹å¼, ä¹Ÿå¯ä»¥é€šè¿‡dockeræ¥è·å–æˆ‘ä»¬å·²ç»ä¸ºä½ é…ç½®å¥½çš„PyPOTSå¼€å‘ç¯å¢ƒå®¹å™¨\n",
    "\n",
    "### ä»ä¸‹æ–¹é€‰æ‹©ä¸€ç§ä½ ç†Ÿæ‚‰çš„å®‰è£…æ–¹å¼æ¥ä¸ºPyPOTSé…ç½®Pythonå¼€å‘ç¯å¢ƒ\n",
    "\n",
    "ğŸ’¡ **è¡¥å……è¯´æ˜**ï¼šä¸åŒçš„å®‰è£…æ–¹å¼é€‚ç”¨äºä¸åŒçš„ä½¿ç”¨åœºæ™¯ã€‚\n",
    "\n",
    "- ä»æºç å®‰è£…é€‚åˆå¸Œæœ›è·å–æœ€æ–°ç‰¹æ€§æˆ–å‚ä¸å¼€å‘çš„ç”¨æˆ·ã€‚\n",
    "- pip å®‰è£…é€‚åˆå¤§å¤šæ•°ç”¨æˆ·ï¼Œè·å–çš„æ˜¯å®˜æ–¹å‘å¸ƒçš„ç¨³å®šç‰ˆæœ¬ã€‚\n",
    "- conda å®‰è£…é€‚åˆåœ¨ Anaconda ç¯å¢ƒä¸­ä½¿ç”¨ï¼Œä¾èµ–ç®¡ç†æ›´æ–¹ä¾¿ã€‚\n",
    "- docker å®‰è£…é€‚åˆå¸Œæœ›ä¸€é”®éƒ¨ç½²ç¯å¢ƒçš„ç”¨æˆ·ï¼Œå°¤å…¶é€‚åˆåœ¨æœåŠ¡å™¨ä¸Šéƒ¨ç½²è¿è¡Œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6788a65-4799-4287-b31e-1c36fe3ff445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»æºç å®‰è£…\n",
    "# ğŸš€ ä»æºç å®‰è£… PyPOTSï¼šæ¨èç”¨äºè·å–æœ€æ–°å¼€å‘ç‰ˆï¼ˆå¯èƒ½åŒ…å«æœ€æ–°ç‰¹æ€§ï¼Œä½†ç¨³å®šæ€§ç•¥ä½ï¼‰\n",
    "# å¦‚æœä½ å¸Œæœ›å‚ä¸å¼€å‘æˆ–æƒ³å°è¯•æœ€æ–°åŠŸèƒ½ï¼Œå¯ä»¥ä½¿ç”¨è¿™ç§æ–¹å¼å®‰è£…\n",
    "!pip install https://github.com/WenjieDu/PyPOTS/archive/main.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6d6a3-d4ad-4e91-8aa2-d3f5eafb6b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»PyPIå®‰è£… \n",
    "# âœ… ä»PyPIå®‰è£… PyPOTSï¼šæ¨èç”¨äºç¨³å®šç‰ˆæœ¬çš„å®‰è£…\n",
    "# è¿™ç§æ–¹å¼é€‚åˆå¤§å¤šæ•°ç”¨æˆ·ï¼Œå®‰è£…çš„æ˜¯åœ¨ PyPI ä¸Šå‘å¸ƒçš„æœ€æ–°ç¨³å®šç‰ˆæœ¬\n",
    "!pip install pypots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc58e92a-80b6-4e44-9372-8ec210b598df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»conda-forgeå®‰è£… (â€¼ï¸è¯·ç¡®å®šä½ ç†Ÿæ‚‰condaçš„æ“ä½œå¹¶ä¸”ç¡®è®¤ä½ çš„ç”µè„‘ä¸Šå®‰è£…äº†conda)\n",
    "!conda install conda-forge::pypots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85b4d8e-c2cb-48c2-a94c-016102f8c100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œé…ç½®å¥½PyPOTSå¼€å‘ç¯å¢ƒçš„dockerå®¹å™¨ (â€¼ï¸è¯·ç¡®å®šä½ ç†Ÿæ‚‰dockerçš„ä½¿ç”¨å¹¶ä¸”ç¡®è®¤ä½ çš„ç”µè„‘ä¸Šå®‰è£…äº†docker)\n",
    "!docker run -it --name pypots wenjiedu/pypots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a250ac53-01c6-4cea-9a5b-c3f4d2bc3fa5",
   "metadata": {},
   "source": [
    "## 2. æ—¶é—´åºåˆ—æ’è¡¥å·¥ä½œæµ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf423f-898b-4afe-a544-999b963de533",
   "metadata": {},
   "source": [
    "## ç”Ÿæˆä¸€ä¸ªéšæœºçš„æ—¶é—´åºåˆ—æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18fddf59-9cba-435a-9ffe-0ebc1b918e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 15:29:55 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: \n",
      "https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012\n",
      "2025-05-09 15:29:55 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...\n",
      "2025-05-09 15:29:55 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...\n",
      "2025-05-09 15:29:55 [INFO]: Loaded successfully!\n",
      "2025-05-09 15:30:03 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. \n",
      "2025-05-09 15:30:03 [INFO]: 22772 values masked out in the val set as ground truth, take 9.92% of the original observed values\n",
      "2025-05-09 15:30:03 [INFO]: 28895 values masked out in the test set as ground truth, take 10.00% of the original observed values\n",
      "2025-05-09 15:30:03 [INFO]: Total sample number: 3997\n",
      "2025-05-09 15:30:03 [INFO]: Training set size: 2557 (63.97%)\n",
      "2025-05-09 15:30:03 [INFO]: Validation set size: 640 (16.01%)\n",
      "2025-05-09 15:30:03 [INFO]: Test set size: 800 (20.02%)\n",
      "2025-05-09 15:30:03 [INFO]: Number of steps: 48\n",
      "2025-05-09 15:30:03 [INFO]: Number of features: 37\n",
      "2025-05-09 15:30:03 [INFO]: Train set missing rate: 79.72%\n",
      "2025-05-09 15:30:03 [INFO]: Validating set missing rate: 81.80%\n",
      "2025-05-09 15:30:03 [INFO]: Test set missing rate: 81.69%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['n_classes', 'n_steps', 'n_features', 'scaler', 'train_X', 'train_y', 'train_ICUType', 'val_X', 'val_y', 'val_ICUType', 'test_X', 'test_y', 'test_ICUType', 'val_X_ori', 'test_X_ori'])\n"
     ]
    }
   ],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "from benchpots.datasets import preprocess_physionet2012  # å¯¼å…¥physionet2012æ•°æ®é›†é¢„å¤„ç†å‡½æ•°\n",
    "\n",
    "# åŠ è½½å¹¶é¢„å¤„ç†physionet2012æ•°æ®é›†\n",
    "# subset=\"set-a\": ä½¿ç”¨set-aå­é›†\n",
    "# pattern=\"point\": ä½¿ç”¨ç‚¹ç¼ºå¤±æ¨¡å¼\n",
    "# rate=0.1: è®¾ç½®ç¼ºå¤±ç‡ä¸º10%\n",
    "physionet2012_dataset = preprocess_physionet2012(\n",
    "    subset=\"set-a\", \n",
    "    pattern=\"point\", \n",
    "    rate=0.1,\n",
    ")\n",
    "\n",
    "# æ‰“å°æ•°æ®é›†çš„é”®ï¼ŒæŸ¥çœ‹åŒ…å«å“ªäº›æ•°æ®\n",
    "print(physionet2012_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3148238-4842-43df-8cbc-9b3538f2a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥numpyåº“ç”¨äºæ•°å€¼è®¡ç®—\n",
    "import numpy as np\n",
    "\n",
    "# åˆ›å»ºæµ‹è¯•é›†çš„ç¼ºå¤±å€¼æŒ‡ç¤ºæ©ç \n",
    "# é€šè¿‡å¼‚æˆ–è¿ç®—(^)æ¯”è¾ƒåŸå§‹æ•°æ®å’Œç¼ºå¤±æ•°æ®çš„NaNä½ç½®ï¼Œå¾—åˆ°çœŸå®ç¼ºå¤±å€¼çš„ä½ç½®\n",
    "physionet2012_dataset[\"test_X_indicating_mask\"] = np.isnan(physionet2012_dataset[\"test_X\"]) ^ np.isnan(physionet2012_dataset[\"test_X_ori\"])\n",
    "\n",
    "# å°†åŸå§‹æµ‹è¯•æ•°æ®ä¸­çš„NaNå€¼æ›¿æ¢ä¸º0\n",
    "physionet2012_dataset[\"test_X_ori\"] = np.nan_to_num(physionet2012_dataset[\"test_X_ori\"])\n",
    "\n",
    "# æ„å»ºè®­ç»ƒé›†å­—å…¸ï¼ŒåªåŒ…å«è¾“å…¥ç‰¹å¾X\n",
    "train_set = {\n",
    "    \"X\": physionet2012_dataset[\"train_X\"],\n",
    "}\n",
    "\n",
    "# æ„å»ºéªŒè¯é›†å­—å…¸ï¼ŒåŒ…å«è¾“å…¥ç‰¹å¾Xå’ŒåŸå§‹æ•°æ®X_ori\n",
    "val_set = {\n",
    "    \"X\": physionet2012_dataset[\"val_X\"],\n",
    "    \"X_ori\": physionet2012_dataset[\"val_X_ori\"],\n",
    "}\n",
    "\n",
    "# æ„å»ºæµ‹è¯•é›†å­—å…¸ï¼ŒåŒ…å«è¾“å…¥ç‰¹å¾Xå’ŒåŸå§‹æ•°æ®X_ori\n",
    "test_set = {\n",
    "    \"X\": physionet2012_dataset[\"test_X\"],\n",
    "    \"X_ori\": physionet2012_dataset[\"test_X_ori\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39a8eff3-d5f9-4c38-9aae-0d711d9fb40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è·å–physionet2012æ•°æ®é›†ä¸­ç‰¹å¾çš„æ•°é‡\n",
    "# n_featuresè¡¨ç¤ºæ•°æ®é›†ä¸­æ¯ä¸ªæ—¶é—´æ­¥åŒ…å«çš„å˜é‡/ç‰¹å¾æ•°é‡\n",
    "physionet2012_dataset['n_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eba5d2f2-505a-44bf-9ef3-92c7b0fe3a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 15:30:22 [INFO]: No given device, using default device: cuda\n",
      "2025-05-09 15:30:22 [WARNING]: â€¼ï¸ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-09 15:30:22 [INFO]: Using customized MAE as the training loss function.\n",
      "2025-05-09 15:30:22 [INFO]: Using customized MSE as the validation metric function.\n",
      "2025-05-09 15:30:22 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 218,294\n"
     ]
    }
   ],
   "source": [
    "# å¯¼å…¥SAITSæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ—¶é—´åºåˆ—æ•°æ®æ’è¡¥çš„æ·±åº¦å­¦ä¹ æ¨¡å‹\n",
    "from pypots.imputation import SAITS\n",
    "\n",
    "# åˆå§‹åŒ–SAITSæ¨¡å‹ï¼Œè®¾ç½®æ¨¡å‹å‚æ•°\n",
    "saits = SAITS(\n",
    "    n_steps=physionet2012_dataset['n_steps'],      # æ—¶é—´æ­¥é•¿ï¼Œä»æ•°æ®é›†ä¸­è·å–\n",
    "    n_features=physionet2012_dataset['n_features'], # ç‰¹å¾æ•°é‡ï¼Œä»æ•°æ®é›†ä¸­è·å–\n",
    "    n_layers=3,                                     # Transformerç¼–ç å™¨å±‚æ•°\n",
    "    d_model=64,                                     # æ¨¡å‹ç»´åº¦\n",
    "    n_heads=4,                                      # æ³¨æ„åŠ›å¤´æ•°\n",
    "    d_k=16,                                         # æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„é”®ç»´åº¦\n",
    "    d_v=16,                                         # æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„å€¼ç»´åº¦\n",
    "    d_ffn=128,                                      # å‰é¦ˆç¥ç»ç½‘ç»œéšè—å±‚ç»´åº¦\n",
    "    dropout=0.1,                                    # Dropoutæ¯”ç‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "    epochs=10,                                      # è®­ç»ƒè½®æ•°\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b6c8f38-54f2-43c2-a011-5e80361ff11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 15:30:35 [INFO]: Epoch 001 - training loss (MAE): 1.1070, validation MSE: 1.2067\n",
      "2025-05-09 15:30:39 [INFO]: Epoch 002 - training loss (MAE): 0.7917, validation MSE: 1.1190\n",
      "2025-05-09 15:30:42 [INFO]: Epoch 003 - training loss (MAE): 0.6935, validation MSE: 1.0924\n",
      "2025-05-09 15:30:45 [INFO]: Epoch 004 - training loss (MAE): 0.6501, validation MSE: 1.0746\n",
      "2025-05-09 15:30:49 [INFO]: Epoch 005 - training loss (MAE): 0.6189, validation MSE: 1.0564\n",
      "2025-05-09 15:30:52 [INFO]: Epoch 006 - training loss (MAE): 0.5973, validation MSE: 1.0546\n",
      "2025-05-09 15:30:55 [INFO]: Epoch 007 - training loss (MAE): 0.5858, validation MSE: 1.0432\n",
      "2025-05-09 15:30:59 [INFO]: Epoch 008 - training loss (MAE): 0.5705, validation MSE: 1.0403\n",
      "2025-05-09 15:31:02 [INFO]: Epoch 009 - training loss (MAE): 0.5576, validation MSE: 1.0374\n",
      "2025-05-09 15:31:06 [INFO]: Epoch 010 - training loss (MAE): 0.5527, validation MSE: 1.0315\n",
      "2025-05-09 15:31:06 [INFO]: Finished training. The best model is from epoch#10.\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨è®­ç»ƒé›†å’ŒéªŒè¯é›†å¯¹SAITSæ¨¡å‹è¿›è¡Œè®­ç»ƒ\n",
    "# train_set: åŒ…å«è®­ç»ƒæ•°æ®çš„å­—å…¸ï¼Œå…¶ä¸­\"X\"é”®å¯¹åº”çš„å€¼åŒ…å«è¾“å…¥ç‰¹å¾\n",
    "# val_set: åŒ…å«éªŒè¯æ•°æ®çš„å­—å…¸ï¼Œå…¶ä¸­\"X\"é”®å¯¹åº”çš„å€¼åŒ…å«è¾“å…¥ç‰¹å¾ï¼Œ\"X_ori\"é”®å¯¹åº”çš„å€¼åŒ…å«åŸå§‹æ•°æ®\n",
    "# è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè‡ªåŠ¨ä½¿ç”¨ä¹‹å‰è®¾ç½®çš„å‚æ•°ï¼ŒåŒ…æ‹¬epochs=10ç­‰è¶…å‚æ•°\n",
    "saits.fit(train_set, val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31b4855c-7965-4b21-958a-4f9be240c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨è®­ç»ƒå¥½çš„SAITSæ¨¡å‹å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼Œç”Ÿæˆç¼ºå¤±å€¼å¡«å……ç»“æœ\n",
    "# test_set: åŒ…å«æµ‹è¯•æ•°æ®çš„å­—å…¸ï¼Œå…¶ä¸­\"X\"é”®å¯¹åº”çš„å€¼åŒ…å«è¾“å…¥ç‰¹å¾\n",
    "# è¿”å›çš„test_set_imputation_resultsæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­é‡è¦çš„é”®å€¼å¯¹ä¸ºï¼š\n",
    "# - \"imputation\": å¡«å……åçš„å®Œæ•´æ—¶é—´åºåˆ—æ•°æ®ï¼Œå½¢çŠ¶ä¸º(n_samples, n_steps, n_features)\n",
    "test_set_imputation_results = saits.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fb7db21-b17a-4634-b940-fbd6f325cdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAITS test_MSE: 0.14154181286514345\n"
     ]
    }
   ],
   "source": [
    "# ä»pypotsåº“ä¸­å¯¼å…¥è®¡ç®—å‡æ–¹è¯¯å·®(MSE)çš„å‡½æ•°\n",
    "from pypots.nn.functional import calc_mse\n",
    "\n",
    "# è®¡ç®—æµ‹è¯•é›†ä¸Šçš„å‡æ–¹è¯¯å·®(MSE)\n",
    "# test_set_imputation_results[\"imputation\"]: SAITSæ¨¡å‹å¯¹æµ‹è¯•é›†çš„æ’è¡¥ç»“æœ\n",
    "# physionet2012_dataset[\"test_X_ori\"]: æµ‹è¯•é›†çš„åŸå§‹å®Œæ•´æ•°æ®\n",
    "# physionet2012_dataset[\"test_X_indicating_mask\"]: æµ‹è¯•é›†çš„ç¼ºå¤±å€¼æŒ‡ç¤ºæ©ç \n",
    "test_MSE = calc_mse(\n",
    "            test_set_imputation_results[\"imputation\"],\n",
    "            physionet2012_dataset[\"test_X_ori\"],\n",
    "            physionet2012_dataset[\"test_X_indicating_mask\"],\n",
    ")\n",
    "# æ‰“å°SAITSæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„MSEè¯„ä¼°ç»“æœ\n",
    "print(f\"SAITS test_MSE: {test_MSE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709f7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "from typing import Optional  # ç”¨äºç±»å‹æç¤º\n",
    "import matplotlib.pyplot as plt  # ç”¨äºç»˜å›¾\n",
    "import numpy as np  # ç”¨äºæ•°å€¼è®¡ç®—\n",
    "import pandas as pd  # ç”¨äºæ•°æ®å¤„ç†\n",
    "from pypots.utils.logging import logger  # ç”¨äºæ—¥å¿—è®°å½•\n",
    "\n",
    "# âš ï¸ TODO: ä¼˜åŒ–è¯¥ç”»å›¾å‡½æ•°\n",
    "def plot_data(\n",
    "    X: np.ndarray,  # è¾“å…¥æ•°æ®ï¼ŒåŒ…å«ç¼ºå¤±å€¼çš„æ—¶é—´åºåˆ—\n",
    "    X_ori: np.ndarray,  # åŸå§‹å®Œæ•´æ•°æ®\n",
    "    X_imputed: np.ndarray,  # æ¨¡å‹å¡«å……åçš„æ•°æ®\n",
    "    sample_idx: Optional[int] = None,  # è¦å¯è§†åŒ–çš„æ ·æœ¬ç´¢å¼•ï¼Œå¦‚æœä¸ºNoneåˆ™éšæœºé€‰æ‹©\n",
    "    n_rows: int = 10,  # å­å›¾çš„è¡Œæ•°\n",
    "    n_cols: int = 4,  # å­å›¾çš„åˆ—æ•°\n",
    "    fig_size: Optional[list] = None,  # å›¾å½¢å¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼\n",
    "):\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–æ—¶é—´åºåˆ—æ•°æ®çš„å‡½æ•°ï¼Œç”¨äºæ¯”è¾ƒåŸå§‹æ•°æ®ã€ç¼ºå¤±æ•°æ®å’Œå¡«å……åçš„æ•°æ®\n",
    "    \n",
    "    å‚æ•°:\n",
    "        X: åŒ…å«ç¼ºå¤±å€¼çš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œå½¢çŠ¶ä¸º(n_samples, n_steps, n_features)\n",
    "        X_ori: åŸå§‹å®Œæ•´æ•°æ®ï¼Œå½¢çŠ¶ä¸Xç›¸åŒ\n",
    "        X_imputed: æ¨¡å‹å¡«å……åçš„æ•°æ®ï¼Œå½¢çŠ¶ä¸Xç›¸åŒ\n",
    "        sample_idx: è¦å¯è§†åŒ–çš„æ ·æœ¬ç´¢å¼•\n",
    "        n_rows: å­å›¾çš„è¡Œæ•°\n",
    "        n_cols: å­å›¾çš„åˆ—æ•°\n",
    "        fig_size: å›¾å½¢å¤§å°\n",
    "    \"\"\"\n",
    "    # è·å–æ•°æ®å½¢çŠ¶\n",
    "    vals_shape = X.shape\n",
    "    assert len(vals_shape) == 3, \"vals_obsåº”è¯¥æ˜¯ä¸€ä¸ª3Dæ•°ç»„ï¼Œå½¢çŠ¶ä¸º(n_samples, n_steps, n_features)\"\n",
    "    n_samples, n_steps, n_features = vals_shape\n",
    "\n",
    "    # å¦‚æœæœªæŒ‡å®šæ ·æœ¬ç´¢å¼•ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ª\n",
    "    if sample_idx is None:\n",
    "        sample_idx = np.random.randint(low=0, high=n_samples)\n",
    "        logger.warning(f\"âš ï¸ æœªæŒ‡å®šæ ·æœ¬ç´¢å¼•ï¼Œéšæœºé€‰æ‹©æ ·æœ¬ {sample_idx} è¿›è¡Œå¯è§†åŒ–ã€‚\")\n",
    "\n",
    "    # è®¾ç½®é»˜è®¤å›¾å½¢å¤§å°\n",
    "    if fig_size is None:\n",
    "        fig_size = [24, 36]\n",
    "\n",
    "    # è®¡ç®—è¦æ˜¾ç¤ºçš„ç‰¹å¾æ•°é‡\n",
    "    n_k = n_rows * n_cols\n",
    "    K = np.min([n_features, n_k])  # å–ç‰¹å¾æ•°é‡å’Œå­å›¾æ•°é‡çš„è¾ƒå°å€¼\n",
    "    L = n_steps  # æ—¶é—´æ­¥é•¿\n",
    "    \n",
    "    # è®¾ç½®å›¾å½¢å‚æ•°\n",
    "    plt.rcParams[\"font.size\"] = 16\n",
    "    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(fig_size[0], fig_size[1]))\n",
    "\n",
    "    # ä¸ºæ¯ä¸ªç‰¹å¾åˆ›å»ºå­å›¾\n",
    "    for k in range(K):\n",
    "        # åˆ›å»ºæ•°æ®æ¡†ç”¨äºç»˜å›¾\n",
    "        df = pd.DataFrame({\"x\": np.arange(0, L), \"val\": X_imputed[sample_idx, :, k]})  # å¡«å……åçš„æ•°æ®\n",
    "        df1 = pd.DataFrame({\"x\": np.arange(0, L), \"val\": X[sample_idx, :, k]})  # ç¼ºå¤±æ•°æ®\n",
    "        df2 = pd.DataFrame({\"x\": np.arange(0, L), \"val\": X_ori[sample_idx, :, k]})  # åŸå§‹æ•°æ®\n",
    "        \n",
    "        # è®¡ç®—å­å›¾ä½ç½®\n",
    "        row = k // n_cols\n",
    "        col = k % n_cols\n",
    "        \n",
    "        # ç»˜åˆ¶ä¸‰ç§æ•°æ®\n",
    "        axes[row][col].plot(df1.x, df1.val, color=\"r\", marker=\"x\", linestyle=\"None\")  # ç¼ºå¤±æ•°æ®ç”¨çº¢è‰²xæ ‡è®°\n",
    "        axes[row][col].plot(df2.x, df2.val, color=\"b\", marker=\"o\", linestyle=\"None\")  # åŸå§‹æ•°æ®ç”¨è“è‰²oæ ‡è®°\n",
    "        axes[row][col].plot(df.x, df.val, color=\"g\", linestyle=\"solid\")  # å¡«å……æ•°æ®ç”¨ç»¿è‰²å®çº¿\n",
    "        \n",
    "        # è®¾ç½®åæ ‡è½´æ ‡ç­¾\n",
    "        if col == 0:\n",
    "            plt.setp(axes[row, 0], ylabel=\"value\")  # ç¬¬ä¸€åˆ—æ·»åŠ yè½´æ ‡ç­¾\n",
    "        if row == -1:\n",
    "            plt.setp(axes[-1, col], xlabel=\"time\")  # æœ€åä¸€è¡Œæ·»åŠ xè½´æ ‡ç­¾\n",
    "\n",
    "    logger.info(\"ç»˜å›¾å®Œæˆã€‚è¯·è°ƒç”¨matplotlib.pyplot.show()æ˜¾ç¤ºå›¾å½¢ã€‚\")\n",
    "\n",
    "\n",
    "# è°ƒç”¨ç»˜å›¾å‡½æ•°ï¼Œå¯è§†åŒ–æµ‹è¯•é›†æ•°æ®\n",
    "plot_data(\n",
    "    test_set[\"X\"],  # æµ‹è¯•é›†è¾“å…¥æ•°æ®\n",
    "    test_set[\"X_ori\"],  # æµ‹è¯•é›†åŸå§‹æ•°æ®\n",
    "    test_set_imputation_results[\"imputation\"],  # æ¨¡å‹å¡«å……ç»“æœ\n",
    "    5,  # é€‰æ‹©ç¬¬5ä¸ªæ ·æœ¬\n",
    "    n_rows=7,  # 7è¡Œå­å›¾\n",
    "    n_cols=6,  # 6åˆ—å­å›¾\n",
    "    fig_size=[100, 50]  # å›¾å½¢å¤§å°\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
